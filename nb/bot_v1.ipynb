{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Load the preprocessed dialogue pairs\n",
    "with open('../data/preprocessed_dialogue_pairs.pkl', 'rb') as file:\n",
    "    preprocessed_dialogue_pairs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['can', 'we', 'make', 'this', 'quick', 'roxanne', 'korrine', 'and', 'andrew', 'barrett', 'are', 'having', 'an', 'incredibly', 'horrendous', 'public', 'break', 'up', 'on', 'the', 'quad', 'again'], ['well', 'i', 'thought', 'we', 'would', 'start', 'with', 'pronunciation', 'if', 'that', 'is', 'okay', 'with', 'you'])\n",
      "(['well', 'i', 'thought', 'we', 'would', 'start', 'with', 'pronunciation', 'if', 'that', 'is', 'okay', 'with', 'you'], ['not', 'the', 'hacking', 'and', 'gagging', 'and', 'spitting', 'part', 'please'])\n",
      "(['not', 'the', 'hacking', 'and', 'gagging', 'and', 'spitting', 'part', 'please'], ['okay', 'then', 'how', 'bout', 'we', 'try', 'out', 'some', 'french', 'cuisine', 'saturday', 'night'])\n",
      "(['you', 'are', 'asking', 'me', 'out', 'that', 'is', 'so', 'cute', 'what', 'is', 'your', 'name', 'again'], ['forget', 'it'])\n",
      "(['the', 'thing', 'is', 'cameron', 'i', 'am', 'at', 'the', 'mercy', 'of', 'a', 'particularly', 'hideous', 'breed', 'of', 'loser', 'my', 'sister', 'i', 'can', 'not', 'date', 'until', 'she', 'does'], ['seems', 'like', 'she', 'could', 'get', 'a', 'date', 'easy', 'enough'])\n",
      "(['unsolved', 'mystery', 'she', 'used', 'to', 'be', 'really', 'popular', 'when', 'she', 'started', 'high', 'school', 'then', 'it', 'was', 'just', 'like', 'she', 'got', 'sick', 'of', 'it', 'or', 'something'], ['that', 'is', 'a', 'shame'])\n",
      "(['gosh', 'if', 'only', 'we', 'could', 'find', 'kat', 'a', 'boyfriend'], ['let', 'me', 'see', 'what', 'i', 'can', 'do'])\n",
      "(['cesc', 'ma', 'tete', 'this', 'is', 'my', 'head'], ['right', 'see', 'you', 'are', 'ready', 'for', 'the', 'quiz'])\n",
      "(['right', 'see', 'you', 'are', 'ready', 'for', 'the', 'quiz'], ['i', 'do', 'not', 'want', 'to', 'know', 'how', 'to', 'say', 'that', 'though', 'i', 'want', 'to', 'know', 'useful', 'things', 'like', 'where', 'the', 'good', 'stores', 'are', 'how', 'much', 'does', 'champagne', 'cost', 'stuff', 'like', 'chat', 'i', 'have', 'never', 'in', 'my', 'life', 'had', 'to', 'point', 'out', 'my', 'head', 'to', 'someone'])\n",
      "(['i', 'do', 'not', 'want', 'to', 'know', 'how', 'to', 'say', 'that', 'though', 'i', 'want', 'to', 'know', 'useful', 'things', 'like', 'where', 'the', 'good', 'stores', 'are', 'how', 'much', 'does', 'champagne', 'cost', 'stuff', 'like', 'chat', 'i', 'have', 'never', 'in', 'my', 'life', 'had', 'to', 'point', 'out', 'my', 'head', 'to', 'someone'], ['that', 'is', 'because', 'it', 'is', 'such', 'a', 'nice', 'one'])\n"
     ]
    }
   ],
   "source": [
    "for pair in preprocessed_dialogue_pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs, val_pairs = train_test_split(preprocessed_dialogue_pairs, test_size=0.2, random_state=42)\n",
    "\n",
    "input_sequences = [pair[0] for pair in train_pairs]\n",
    "target_sequences = [pair[1] for pair in train_pairs]\n",
    "\n",
    "# Create a generator function to yield tokens\n",
    "def yield_tokens(tokenized_sequences):\n",
    "    for sequence in tokenized_sequences:\n",
    "        yield sequence\n",
    "\n",
    "# Create vocabulary mappings for the input and target sequences\n",
    "special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "input_vocab = build_vocab_from_iterator(yield_tokens(input_sequences), specials=special_tokens)\n",
    "target_vocab = build_vocab_from_iterator(yield_tokens(target_sequences), specials=special_tokens)\n",
    "\n",
    "# Set the default index for handling unknown tokens\n",
    "input_vocab.set_default_index(input_vocab['<unk>'])\n",
    "target_vocab.set_default_index(target_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 0, Index: <pad>\n",
      "Token: 1, Index: <sos>\n",
      "Token: 2, Index: <eos>\n",
      "Token: 3, Index: <unk>\n",
      "Token: 4, Index: you\n",
      "Token: 5, Index: i\n",
      "Token: 6, Index: the\n",
      "Token: 7, Index: to\n",
      "Token: 8, Index: is\n",
      "Token: 9, Index: not\n",
      "Token: 0, Index: <pad>\n",
      "Token: 1, Index: <sos>\n",
      "Token: 2, Index: <eos>\n",
      "Token: 3, Index: <unk>\n",
      "Token: 4, Index: i\n",
      "Token: 5, Index: you\n",
      "Token: 6, Index: the\n",
      "Token: 7, Index: to\n",
      "Token: 8, Index: is\n",
      "Token: 9, Index: not\n"
     ]
    }
   ],
   "source": [
    "for token, index in enumerate(input_vocab.get_itos()[:10]):\n",
    "    print(f\"Token: {token}, Index: {index}\")\n",
    "for token, index in enumerate(target_vocab.get_itos()[:10]):\n",
    "    print(f\"Token: {token}, Index: {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered First List Length Percentages: [(2, 6.7464), (3, 7.816), (4, 9.6323), (5, 8.9494), (6, 7.981), (7, 6.9068), (8, 5.8006), (9, 4.8594), (10, 4.1938), (11, 3.714), (12, 3.2554), (13, 2.9479), (14, 2.5911), (15, 2.3475), (16, 1.9788), (17, 1.8916), (18, 1.6154), (19, 1.437), (20, 1.3585), (21, 1.2067), (22, 1.1202), (23, 0.9445), (24, 0.8872), (25, 0.7528), (26, 0.7368), (27, 0.6603), (28, 0.6203), (29, 0.5618), (30, 0.4726), (31, 0.4406), (32, 0.4226), (33, 0.3634), (34, 0.3534), (35, 0.3208), (36, 0.3108), (37, 0.2576), (38, 0.2409), (39, 0.2283), (40, 0.2077), (41, 0.1963), (42, 0.1924), (43, 0.1671), (44, 0.1564), (45, 0.1524), (46, 0.1251), (47, 0.1171), (48, 0.1318), (49, 0.1025), (50, 0.1052), (51, 0.0872), (52, 0.0972), (53, 0.0706), (54, 0.0626), (55, 0.0699), (56, 0.0626), (57, 0.0599), (58, 0.0526), (59, 0.0599), (60, 0.0606), (61, 0.0459), (62, 0.0433), (63, 0.0373), (64, 0.0373), (65, 0.0346), (66, 0.0366), (67, 0.0386), (68, 0.0293), (69, 0.0306), (70, 0.02), (71, 0.0253), (72, 0.0193), (73, 0.0293), (74, 0.0253), (75, 0.02), (76, 0.018), (77, 0.0146), (78, 0.0126), (79, 0.0146), (80, 0.0133), (81, 0.0153), (82, 0.01), (83, 0.012), (84, 0.0133), (85, 0.0113), (86, 0.0093), (87, 0.0153), (88, 0.0133), (89, 0.006), (90, 0.0093), (91, 0.0053), (92, 0.0106), (93, 0.0053), (94, 0.0126), (95, 0.008), (96, 0.01), (97, 0.008), (98, 0.0067), (99, 0.0073), (100, 0.004)]\n",
      "Cumulative First List Length Percentages: [(2, 6.7464), (3, 14.5623), (4, 24.1946), (5, 33.1441), (6, 41.1251), (7, 48.0319), (8, 53.8324), (9, 58.6919), (10, 62.8857), (11, 66.5997), (12, 69.855), (13, 72.8029), (14, 75.394), (15, 77.7415), (16, 79.7203), (17, 81.6119), (18, 83.2273), (19, 84.6643), (20, 86.0227), (21, 87.2294), (22, 88.3496), (23, 89.2941), (24, 90.1813), (25, 90.9341), (26, 91.6709), (27, 92.3311), (28, 92.9515), (29, 93.5132), (30, 93.9858), (31, 94.4264), (32, 94.849), (33, 95.2125), (34, 95.5659), (35, 95.8867), (36, 96.1975), (37, 96.4551), (38, 96.696), (39, 96.9243), (40, 97.132), (41, 97.3283), (42, 97.5207), (43, 97.6878), (44, 97.8442), (45, 97.9966), (46, 98.1217), (47, 98.2389), (48, 98.3707), (49, 98.4732), (50, 98.5783), (51, 98.6655), (52, 98.7627), (53, 98.8332), (54, 98.8958), (55, 98.9657), (56, 99.0282), (57, 99.0881), (58, 99.1407), (59, 99.2006), (60, 99.2612), (61, 99.3071), (62, 99.3504), (63, 99.3877), (64, 99.4249), (65, 99.4595), (66, 99.4962), (67, 99.5348), (68, 99.564), (69, 99.5947), (70, 99.6146), (71, 99.6399), (72, 99.6592), (73, 99.6885), (74, 99.7138), (75, 99.7338), (76, 99.7517), (77, 99.7664), (78, 99.779), (79, 99.7937), (80, 99.807), (81, 99.8223), (82, 99.8323), (83, 99.8443), (84, 99.8576), (85, 99.8689), (86, 99.8782), (87, 99.8935), (88, 99.9068), (89, 99.9128), (90, 99.9221), (91, 99.9275), (92, 99.9381), (93, 99.9434), (94, 99.9561), (95, 99.9641), (96, 99.974), (97, 99.982), (98, 99.9887), (99, 99.996), (100, 100.0)]\n",
      "Ordered Second List Length Percentages: [(2, 7.3494), (3, 7.9105), (4, 9.1751), (5, 8.387), (6, 7.563), (7, 6.7164), (8, 5.5862), (9, 4.8621), (10, 4.1113), (11, 3.666), (12, 3.2893), (13, 3.0477), (14, 2.6311), (15, 2.3043), (16, 2.0806), (17, 1.8883), (18, 1.6813), (19, 1.5202), (20, 1.3711), (21, 1.2273), (22, 1.1455), (23, 0.997), (24, 0.9378), (25, 0.84), (26, 0.8094), (27, 0.6729), (28, 0.6436), (29, 0.5751), (30, 0.4939), (31, 0.4726), (32, 0.438), (33, 0.3867), (34, 0.3614), (35, 0.3468), (36, 0.3235), (37, 0.2756), (38, 0.2489), (39, 0.2236), (40, 0.2323), (41, 0.2156), (42, 0.2117), (43, 0.1677), (44, 0.175), (45, 0.1864), (46, 0.1344), (47, 0.1165), (48, 0.1371), (49, 0.1191), (50, 0.0899), (51, 0.0945), (52, 0.1018), (53, 0.0992), (54, 0.0752), (55, 0.0732), (56, 0.0745), (57, 0.0632), (58, 0.0473), (59, 0.0632), (60, 0.0586), (61, 0.0499), (62, 0.0479), (63, 0.0379), (64, 0.0526), (65, 0.0379), (66, 0.0473), (67, 0.0399), (68, 0.0413), (69, 0.0359), (70, 0.0273), (71, 0.0306), (72, 0.0246), (73, 0.0253), (74, 0.0233), (75, 0.0226), (76, 0.0226), (77, 0.024), (78, 0.0193), (79, 0.018), (80, 0.0193), (81, 0.0206), (82, 0.0226), (83, 0.02), (84, 0.0093), (85, 0.0113), (86, 0.016), (87, 0.014), (88, 0.0166), (89, 0.014), (90, 0.014), (91, 0.0113), (92, 0.0073), (93, 0.0126), (94, 0.014), (95, 0.012), (96, 0.0126), (97, 0.0087), (98, 0.0073), (99, 0.0053), (100, 0.006)]\n",
      "Cumulative Second List Length Percentages: [(2, 7.3494), (3, 15.2598), (4, 24.4349), (5, 32.8219), (6, 40.385), (7, 47.1014), (8, 52.6876), (9, 57.5497), (10, 61.661), (11, 65.3271), (12, 68.6164), (13, 71.6641), (14, 74.2951), (15, 76.5994), (16, 78.68), (17, 80.5683), (18, 82.2495), (19, 83.7697), (20, 85.1408), (21, 86.3682), (22, 87.5136), (23, 88.5107), (24, 89.4485), (25, 90.2885), (26, 91.0978), (27, 91.7707), (28, 92.4143), (29, 92.9894), (30, 93.4833), (31, 93.9558), (32, 94.3938), (33, 94.7805), (34, 95.1419), (35, 95.4887), (36, 95.8121), (37, 96.0877), (38, 96.3366), (39, 96.5603), (40, 96.7926), (41, 97.0082), (42, 97.2199), (43, 97.3876), (44, 97.5626), (45, 97.749), (46, 97.8834), (47, 97.9999), (48, 98.137), (49, 98.2562), (50, 98.346), (51, 98.4405), (52, 98.5424), (53, 98.6415), (54, 98.7168), (55, 98.79), (56, 98.8645), (57, 98.9277), (58, 98.975), (59, 99.0382), (60, 99.0968), (61, 99.1467), (62, 99.1946), (63, 99.2326), (64, 99.2852), (65, 99.3231), (66, 99.3704), (67, 99.4103), (68, 99.4516), (69, 99.4875), (70, 99.5148), (71, 99.5454), (72, 99.57), (73, 99.5953), (74, 99.6186), (75, 99.6413), (76, 99.6639), (77, 99.6878), (78, 99.7071), (79, 99.7251), (80, 99.7444), (81, 99.765), (82, 99.7877), (83, 99.8076), (84, 99.817), (85, 99.8283), (86, 99.8443), (87, 99.8582), (88, 99.8749), (89, 99.8888), (90, 99.9028), (91, 99.9141), (92, 99.9215), (93, 99.9341), (94, 99.9481), (95, 99.9601), (96, 99.9727), (97, 99.9814), (98, 99.9887), (99, 99.994), (100, 100.0)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "first_list_lengths = [len(pair[0]) for pair in train_pairs]\n",
    "second_list_lengths = [len(pair[1]) for pair in train_pairs]\n",
    "\n",
    "# Count occurrences of each length\n",
    "first_list_length_counts = Counter(first_list_lengths)\n",
    "second_list_length_counts = Counter(second_list_lengths)\n",
    "\n",
    "# Calculate percentages\n",
    "total_first_list = len(first_list_lengths)\n",
    "total_second_list = len(second_list_lengths)\n",
    "\n",
    "first_list_length_percentages = {length: count / total_first_list * 100 for length, count in first_list_length_counts.items()}\n",
    "second_list_length_percentages = {length: count / total_second_list * 100 for length, count in second_list_length_counts.items()}\n",
    "\n",
    "# Order by length\n",
    "ordered_first_list_percentages = sorted(first_list_length_percentages.items(), key=lambda x: x[0])\n",
    "ordered_second_list_percentages = sorted(second_list_length_percentages.items(), key=lambda x: x[0])\n",
    "\n",
    "# Calculate cumulative percentages\n",
    "cumulative_first_list_percentages = []\n",
    "cumulative_percentage = 0\n",
    "for length, percentage in ordered_first_list_percentages:\n",
    "    cumulative_percentage += percentage\n",
    "    cumulative_first_list_percentages.append((length, cumulative_percentage))\n",
    "\n",
    "cumulative_second_list_percentages = []\n",
    "cumulative_percentage = 0\n",
    "for length, percentage in ordered_second_list_percentages:\n",
    "    cumulative_percentage += percentage\n",
    "    cumulative_second_list_percentages.append((length, cumulative_percentage))\n",
    "\n",
    "# Print the ordered and cumulative percentages\n",
    "print(\"Ordered First List Length Percentages:\", [(length, round(percentage, 4)) for length, percentage in ordered_first_list_percentages])\n",
    "print(\"Cumulative First List Length Percentages:\", [(length, round(percentage, 4)) for length, percentage in cumulative_first_list_percentages])\n",
    "\n",
    "print(\"Ordered Second List Length Percentages:\", [(length, round(percentage, 4)) for length, percentage in ordered_second_list_percentages])\n",
    "print(\"Cumulative Second List Length Percentages:\", [(length, round(percentage, 4)) for length, percentage in cumulative_second_list_percentages])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers, dropout_p = 0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.dropout(self.embedding(input_seq))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden, decoder_input, max_length, target_tensor = None, teaching_force_ratio = 0.5):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_outputs = []\n",
    "        decoder_hidden = hidden \n",
    "\n",
    "        for i in range(max_length):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            #decoder_output = decoder_output.detach()\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            if target_tensor is not None and random.random() < teaching_force_ratio:\n",
    "                # Teacher forcing\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1).detach()\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46276\n",
      "46616\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(input_vocab)\n",
    "output_dim = len(target_vocab)\n",
    "emb_dim = 128\n",
    "hidden_dim = 128\n",
    "num_layers = 1\n",
    "MAX_LEN = 100\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "batch_size = 32  # Adjust the batch size as per your requirements\n",
    "print(input_dim)\n",
    "print(output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# # Clear GPU memory cache\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Iterate over all available GPUs\n",
    "# for i in range(torch.cuda.device_count()):\n",
    "#     current_device = torch.device(f'cuda:{i}')\n",
    "#     print(current_device)\n",
    "#     # Move to the current GPU\n",
    "#     with torch.cuda.device(current_device):\n",
    "#         # Iterate over model parameters and buffers\n",
    "#         for obj in list(torch.nn.Module().parameters()) + list(torch.nn.Module().buffers()):\n",
    "#             print(obj)\n",
    "#             if obj is not None and obj.is_cuda:\n",
    "#                 obj.data = None  # This will release the memory associated with the tensor\n",
    "\n",
    "# # Optionally, clear the GPU memory cache again\n",
    "# torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1573"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150244\n",
      "4696\n",
      "37561\n",
      "1174\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the encoder and decoder and move them to the appropriate device\n",
    "encoder = Encoder(input_dim, emb_dim, hidden_dim, num_layers).to(device)\n",
    "decoder = Decoder(output_dim, emb_dim, hidden_dim, num_layers).to(device)\n",
    "\n",
    "# Define the optimizer and move the parameters to the appropriate device\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss(ignore_index=target_vocab['<pad>'])\n",
    "\n",
    "# Create batches of train pairs\n",
    "train_batches = [train_pairs[i:i+batch_size] for i in range(0, len(train_pairs), batch_size)]\n",
    "val_batches = [val_pairs[i:i+batch_size] for i in range(0, len(val_pairs), batch_size)]\n",
    "print(len(train_pairs))\n",
    "print(len(train_batches))\n",
    "print(len(val_pairs))\n",
    "print(len(val_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 30.13s, Training Visited 10000 lines, Loss: 0.1897\n",
      "Time: 58.32s, Training Visited 20000 lines, Loss: 0.1831\n",
      "Time: 85.87s, Training Visited 30000 lines, Loss: 0.1793\n",
      "Time: 113.02s, Training Visited 40000 lines, Loss: 0.1770\n",
      "Time: 139.15s, Training Visited 50000 lines, Loss: 0.1753\n",
      "Time: 165.79s, Training Visited 60000 lines, Loss: 0.1740\n",
      "Time: 193.54s, Training Visited 70000 lines, Loss: 0.1730\n",
      "Time: 222.22s, Training Visited 80000 lines, Loss: 0.1721\n",
      "Time: 249.79s, Training Visited 90000 lines, Loss: 0.1714\n",
      "Time: 275.33s, Training Visited 100000 lines, Loss: 0.1708\n",
      "Time: 302.11s, Training Visited 110000 lines, Loss: 0.1702\n",
      "Time: 329.26s, Training Visited 120000 lines, Loss: 0.1697\n",
      "Time: 357.27s, Training Visited 130000 lines, Loss: 0.1693\n",
      "Time: 384.03s, Training Visited 140000 lines, Loss: 0.1689\n",
      "Time: 411.45s, Training Visited 150000 lines, Loss: 0.1686\n",
      "Time: 412.16s, Epoch: 1, Training Loss: 0.1686\n",
      "Time: 426.35s, Validation Visited 10000 lines, Loss: 0.1774\n",
      "Time: 440.42s, Validation Visited 20000 lines, Loss: 0.1776\n",
      "Time: 454.42s, Validation Visited 30000 lines, Loss: 0.1776\n",
      "Time: 465.04s, Epoch: 1, Validation Loss: 0.0444\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in train_batches[:]:\n",
    "        counter += batch_size\n",
    "        if counter % 10000 < batch_size:\n",
    "            end_time = time.time()\n",
    "            time_diff = end_time - start_time\n",
    "            average_loss = total_loss / counter\n",
    "            print(f\"Time: {time_diff:.2f}s, Training Visited {counter // 10000 * 10000} lines, Loss: {average_loss:.4f}\")\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        input_seqs = [pair[0] + ['<eos>'] for pair in batch]\n",
    "        target_seqs = [['<sos>'] + pair[1] + ['<eos>'] for pair in batch]\n",
    "\n",
    "        input_max_len = max(len(seq) for seq in input_seqs)\n",
    "        target_max_len = max(len(seq) for seq in target_seqs)\n",
    "\n",
    "        input_indices = [[input_vocab[token] for token in seq] + [input_vocab['<pad>']] * (input_max_len - len(seq)) for seq in input_seqs]\n",
    "        target_indices = [[target_vocab[token] for token in seq] + [target_vocab['<pad>']] * (target_max_len - len(seq)) for seq in target_seqs]\n",
    "\n",
    "        input_seq = torch.tensor(input_indices).to(device)\n",
    "        target_seq = torch.tensor(target_indices).to(device)\n",
    "        encoder_outputs, encoder_hidden = encoder(input_seq)\n",
    "        decoder_input = target_seq[:, 0].unsqueeze(1).to(device)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, decoder_input, min(MAX_LEN, target_max_len), target_tensor = target_seq, teaching_force_ratio = 0.5)\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_seq[:, :MAX_LEN].reshape(-1)\n",
    "        )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "    average_loss = total_loss / len(train_pairs)\n",
    "    end_time = time.time()\n",
    "    time_diff = end_time - start_time\n",
    "    print(f\"Time: {time_diff:.2f}s, Epoch: {epoch+1}, Training Loss: {average_loss:.4f}\")\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        counter = 0\n",
    "        for batch in val_batches[:]:\n",
    "            counter += batch_size\n",
    "            if counter % 10000 < batch_size:\n",
    "                end_time = time.time()\n",
    "                time_diff = end_time - start_time\n",
    "                average_loss = total_loss / counter\n",
    "                print(f\"Time: {time_diff:.2f}s, Validation Visited {counter // 10000 * 10000} lines, Loss: {average_loss:.4f}\")\n",
    "\n",
    "            input_seqs = [pair[0] + ['<eos>'] for pair in batch]\n",
    "            target_seqs = [['<sos>'] + pair[1] + ['<eos>'] for pair in batch]\n",
    "\n",
    "            input_max_len = max(len(seq) for seq in input_seqs)\n",
    "            target_max_len = max(len(seq) for seq in target_seqs)\n",
    "\n",
    "            input_indices = [[input_vocab[token] for token in seq] + [input_vocab['<pad>']] * (input_max_len - len(seq)) for seq in input_seqs]\n",
    "            target_indices = [[target_vocab[token] for token in seq] + [target_vocab['<pad>']] * (target_max_len - len(seq)) for seq in target_seqs]\n",
    "\n",
    "            input_seq = torch.tensor(input_indices).to(device)\n",
    "            target_seq = torch.tensor(target_indices).to(device)\n",
    "            encoder_outputs, encoder_hidden = encoder(input_seq)\n",
    "            decoder_input = torch.tensor([target_vocab[\"<sos>\"]] * input_seq.shape[0]).unsqueeze(1).to(device)\n",
    "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, decoder_input, min(MAX_LEN, target_max_len), target_tensor = None)\n",
    "            loss = criterion(\n",
    "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "                target_seq[:, :MAX_LEN].reshape(-1)\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(val_pairs)\n",
    "        end_time = time.time()\n",
    "        time_diff = end_time - start_time\n",
    "        print(f\"Time: {time_diff:.2f}s, Epoch: {epoch+1}, Validation Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     encoder.train()\n",
    "#     decoder.train()\n",
    "#     total_loss = 0\n",
    "#     total_mask = 0\n",
    "#     counter = 0\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     for batch in train_batches[:]:\n",
    "#         counter += batch_size\n",
    "#         if counter % 10000 < batch_size:\n",
    "#             end_time = time.time()\n",
    "#             time_diff = end_time - start_time\n",
    "#             average_loss = total_loss / counter\n",
    "#             print(f\"Time: {time_diff:.2f}s, Training Visited {counter // 10000 * 10000} lines, Loss: {average_loss:.4f}\")\n",
    "\n",
    "#         encoder_optimizer.zero_grad()\n",
    "#         decoder_optimizer.zero_grad()\n",
    "\n",
    "#         # input_seqs = [pair[0] for pair in batch]\n",
    "#         # target_seqs = [pair[1] for pair in batch]\n",
    "#         # Add the <eos> token to the end of each input sequence\n",
    "#         input_seqs = [pair[0] + ['<eos>'] for pair in batch]\n",
    "#         # Add the <sos> token to the beginning and the <eos> token to the end of each target sequence\n",
    "#         target_seqs = [['<sos>'] + pair[1] + ['<eos>'] for pair in batch]\n",
    "\n",
    "#         input_max_len = max(len(seq) for seq in input_seqs)\n",
    "#         target_max_len = max(len(seq) for seq in target_seqs)\n",
    "\n",
    "#         input_indices = [[input_vocab[token] for token in seq] + [input_vocab['<pad>']] * (input_max_len - len(seq)) for seq in input_seqs]\n",
    "#         target_indices = [[target_vocab[token] for token in seq] + [target_vocab['<pad>']] * (target_max_len - len(seq)) for seq in target_seqs]\n",
    "\n",
    "#         input_seq = torch.tensor(input_indices).to(device)\n",
    "#         target_seq = torch.tensor(target_indices).to(device)\n",
    "#         # mask = (target_seq != target_vocab['<pad>']).to(device)\n",
    "\n",
    "#         # print(input_seq.shape)\n",
    "#         # print(target_seq.shape)\n",
    "#         # print(mask.sum())\n",
    "#         encoder_outputs, encoder_hidden = encoder(input_seq)\n",
    "#         decoder_input = target_seq[:, 0].unsqueeze(1).to(device)\n",
    "#         decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, decoder_input, min(MAX_LEN, target_max_len), target_tensor = target_seq, teaching_force_ratio = 0.5)\n",
    "#         loss = criterion(\n",
    "#             decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "#             target_seq[:, :MAX_LEN].reshape(-1)\n",
    "#         )\n",
    "\n",
    "#         # loss = 0\n",
    "#         # for token in range(target_seq.shape[1]):\n",
    "#         #     decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "#         #     # step_loss = criterion(decoder_output, target_seq[:, token])\n",
    "#         #     step_loss = criterion(decoder_output[mask[:, token]], target_seq[:, token][mask[:, token]])\n",
    "#         #     loss += step_loss\n",
    "#         #     decoder_input = target_seq[:, token].unsqueeze(1)\n",
    "#         total_loss += loss.item()\n",
    "#         # total_mask += mask.sum()\n",
    "#         loss.backward()\n",
    "#         encoder_optimizer.step()\n",
    "#         decoder_optimizer.step()\n",
    "\n",
    "#     # average_loss = total_loss / total_mask\n",
    "#     average_loss = total_loss / len(train_pairs)\n",
    "#     # average_loss = total_loss / 10\n",
    "#     end_time = time.time()\n",
    "#     time_diff = end_time - start_time\n",
    "#     print(f\"Time: {time_diff:.2f}s, Epoch: {epoch+1}, Training Loss: {average_loss:.4f}\")\n",
    "\n",
    "#     encoder.eval()\n",
    "#     decoder.eval()\n",
    "#     with torch.no_grad():\n",
    "#         total_loss = 0\n",
    "#         total_mask = 0\n",
    "#         counter = 0\n",
    "#         for batch in val_batches[:]:\n",
    "#             counter += batch_size\n",
    "#             if counter % 10000 < batch_size:\n",
    "#                 end_time = time.time()\n",
    "#                 time_diff = end_time - start_time\n",
    "#                 average_loss = total_loss / counter\n",
    "#                 print(f\"Time: {time_diff:.2f}s, Validation Visited {counter // 10000 * 10000} lines, Loss: {average_loss:.4f}\")\n",
    "\n",
    "#             # input_seqs = [pair[0] for pair in batch]\n",
    "#             # target_seqs = [pair[1] for pair in batch]\n",
    "#             # Add the <eos> token to the end of each input sequence\n",
    "#             input_seqs = [pair[0] + ['<eos>'] for pair in batch]\n",
    "#             # Add the <sos> token to the beginning and the <eos> token to the end of each target sequence\n",
    "#             target_seqs = [['<sos>'] + pair[1] + ['<eos>'] for pair in batch]\n",
    "\n",
    "#             input_max_len = max(len(seq) for seq in input_seqs)\n",
    "#             target_max_len = max(len(seq) for seq in target_seqs)\n",
    "\n",
    "#             input_indices = [[input_vocab[token] for token in seq] + [input_vocab['<pad>']] * (input_max_len - len(seq)) for seq in input_seqs]\n",
    "#             target_indices = [[target_vocab[token] for token in seq] + [target_vocab['<pad>']] * (target_max_len - len(seq)) for seq in target_seqs]\n",
    "\n",
    "#             input_seq = torch.tensor(input_indices).to(device)\n",
    "#             target_seq = torch.tensor(target_indices).to(device)\n",
    "#             # for simplicity, mask validation in the same way. Ignore all <pad>.\n",
    "#             # mask = (target_seq != target_vocab['<pad>']).to(device)\n",
    "#             encoder_outputs, encoder_hidden = encoder(input_seq)\n",
    "#             decoder_input = torch.tensor([target_vocab[\"<sos>\"]] * input_seq.shape[0]).unsqueeze(1).to(device)\n",
    "#             decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, decoder_input, min(MAX_LEN, target_max_len), target_tensor = None)\n",
    "#             loss = criterion(\n",
    "#                 decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "#                 target_seq[:, :MAX_LEN].reshape(-1)\n",
    "#             )\n",
    "\n",
    "#             # loss = 0\n",
    "#             # for token in range(target_seq.shape[1]):\n",
    "#             #     decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "#             #     # step_loss = criterion(decoder_output.squeeze(1), target_seq[:, token])\n",
    "#             #     step_loss = criterion(decoder_output[mask[:, token]], target_seq[:, token][mask[:, token]])\n",
    "#             #     loss += step_loss\n",
    "#             #     decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(1)\n",
    "#             total_loss += loss.item()\n",
    "#             # total_mask += mask.sum()\n",
    "\n",
    "#         # average_loss = total_loss / total_mask\n",
    "#         average_loss = total_loss / len(val_pairs)\n",
    "#         # average_loss = total_loss / 10\n",
    "#         end_time = time.time()\n",
    "#         time_diff = end_time - start_time\n",
    "#         print(f\"Time: {time_diff:.2f}s, Epoch: {epoch+1}, Validation Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\weiyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> i am not <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.vocab import Vocab\n",
    "import contractions\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_and_tokenize_text(text):\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #tokens = [token for token in tokens if token not in stop_words]\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tokens = [token.strip() for token in tokens if token.strip()]\n",
    "    return tokens\n",
    "\n",
    "# Preprocess the input string\n",
    "input_string = 'how are you'\n",
    "tokenized_input = clean_and_tokenize_text(input_string) + ['<eos>']\n",
    "\n",
    "# Convert tokens to indices using the input vocabulary\n",
    "input_seqs = torch.tensor([input_vocab[token] for token in tokenized_input]).unsqueeze(0).to(device) # Shape: [batch_size, sequence_length]\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Pass the input through the encoder\n",
    "encoder_outputs, encoder_hidden = encoder(input_seqs)\n",
    "# Initialize the decoder's input\n",
    "decoder_input = torch.tensor([target_vocab[\"<sos>\"]] * 1).unsqueeze(1).to(device)\n",
    "# decoder_input = torch.tensor([[target_vocab['<sos>']]]).to(device)  # Use the index for the start token\n",
    "decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, decoder_input, min(MAX_LEN, target_max_len), target_tensor = None)\n",
    "\n",
    "_, predicted_indices = torch.max(decoder_outputs, dim=-1)\n",
    "predicted_tokens = [target_vocab.get_itos()[token_idx.item()] for token_idx in predicted_indices[0]]\n",
    "predicted_text = ' '.join(predicted_tokens)\n",
    "print(predicted_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
