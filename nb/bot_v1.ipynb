{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Load the preprocessed dialogue pairs\n",
    "with open('../data/preprocessed_dialogue_pairs.pkl', 'rb') as file:\n",
    "    preprocessed_dialogue_pairs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs, val_pairs = train_test_split(preprocessed_dialogue_pairs, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the tokenizer function\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "input_sequences = [pair[0] for pair in train_pairs]\n",
    "target_sequences = [pair[1] for pair in train_pairs]\n",
    "\n",
    "# Create a generator function to yield tokens\n",
    "def yield_tokens(tokenized_sequences):\n",
    "    for sequence in tokenized_sequences:\n",
    "        yield sequence\n",
    "\n",
    "# Create vocabulary mappings for the input and target sequences\n",
    "special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "input_vocab = build_vocab_from_iterator(yield_tokens(input_sequences), specials=special_tokens)\n",
    "target_vocab = build_vocab_from_iterator(yield_tokens(target_sequences), specials=special_tokens)\n",
    "\n",
    "# Set the default index for handling unknown tokens\n",
    "input_vocab.set_default_index(input_vocab['<unk>'])\n",
    "target_vocab.set_default_index(target_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered First List Length Percentages: [(0, 0.0925), (1, 7.6061), (2, 6.271), (3, 7.297), (4, 8.8791), (5, 8.1335), (6, 7.4459), (7, 6.3816), (8, 5.3279), (9, 4.4508), (10, 3.8614), (11, 3.4062), (12, 2.9567), (13, 2.7632), (14, 2.369), (15, 2.1614), (16, 1.8444), (17, 1.6995), (18, 1.4688), (19, 1.3012), (20, 1.2415), (21, 1.0897), (22, 0.9972), (23, 0.8816), (24, 0.8342), (25, 0.7118), (26, 0.6983), (27, 0.586), (28, 0.5499), (29, 0.5121), (30, 0.4315), (31, 0.4129), (32, 0.3835), (33, 0.3503), (34, 0.3232), (35, 0.2984), (36, 0.2775), (37, 0.2465), (38, 0.2172), (39, 0.2352), (40, 0.1923), (41, 0.1856), (42, 0.1805), (43, 0.1624), (44, 0.132), (45, 0.1393), (46, 0.1207), (47, 0.1021), (48, 0.0987), (49, 0.1066), (50, 0.0914), (51, 0.0908), (52, 0.0773), (53, 0.0705), (54, 0.0643), (55, 0.0564), (56, 0.0592), (57, 0.0615), (58, 0.0575), (59, 0.0463), (60, 0.0513), (61, 0.0434), (62, 0.0406), (63, 0.04), (64, 0.0367), (65, 0.0299), (66, 0.0338), (67, 0.0271), (68, 0.0327), (69, 0.0276), (70, 0.0288), (71, 0.018), (72, 0.0186), (73, 0.0186), (74, 0.0282), (75, 0.0226), (76, 0.0197), (77, 0.0135), (78, 0.0141), (79, 0.018), (80, 0.0096), (81, 0.0096), (82, 0.0152), (83, 0.0118), (84, 0.0096), (85, 0.0102), (86, 0.0102), (87, 0.0124), (88, 0.0079), (89, 0.0102), (90, 0.0096), (91, 0.0056), (92, 0.0102), (93, 0.0079), (94, 0.009), (95, 0.0073), (96, 0.0085), (97, 0.0056), (98, 0.0085), (99, 0.0079), (100, 0.0023), (101, 0.0023), (102, 0.0073), (103, 0.0051), (104, 0.0056), (105, 0.0045), (106, 0.0062), (107, 0.0051), (108, 0.0056), (109, 0.0051), (110, 0.0011), (111, 0.0045), (112, 0.0039), (113, 0.0023), (114, 0.0045), (115, 0.0028), (116, 0.0051), (117, 0.0017), (118, 0.0011), (119, 0.0023), (120, 0.0039), (121, 0.0023), (122, 0.0017), (123, 0.0034), (124, 0.0034), (125, 0.0034), (126, 0.0006), (127, 0.0017), (128, 0.0028), (129, 0.0023), (130, 0.0011), (131, 0.0006), (132, 0.0017), (133, 0.0034), (134, 0.0006), (135, 0.0011), (136, 0.0017), (137, 0.0017), (138, 0.0006), (139, 0.0006), (140, 0.0023), (141, 0.0006), (142, 0.0006), (143, 0.0006), (144, 0.0006), (145, 0.0006), (146, 0.0011), (147, 0.0017), (148, 0.0023), (149, 0.0017), (150, 0.0011), (153, 0.0006), (154, 0.0017), (157, 0.0006), (159, 0.0006), (161, 0.0006), (163, 0.0006), (164, 0.0017), (165, 0.0006), (166, 0.0011), (168, 0.0011), (170, 0.0006), (171, 0.0006), (173, 0.0011), (174, 0.0006), (177, 0.0006), (178, 0.0006), (179, 0.0006), (180, 0.0006), (181, 0.0006), (182, 0.0011), (187, 0.0006), (189, 0.0011), (192, 0.0006), (197, 0.0011), (198, 0.0006), (199, 0.0006), (200, 0.0006), (203, 0.0006), (204, 0.0006), (205, 0.0006), (215, 0.0006), (217, 0.0006), (219, 0.0006), (220, 0.0006), (227, 0.0011), (233, 0.0006), (253, 0.0006), (293, 0.0006), (304, 0.0006), (319, 0.0006)]\n",
      "Cumulative First List Length Percentages: [(0, 0.0925), (1, 7.6986), (2, 13.9696), (3, 21.2666), (4, 30.1457), (5, 38.2792), (6, 45.7251), (7, 52.1067), (8, 57.4346), (9, 61.8855), (10, 65.7469), (11, 69.1531), (12, 72.1099), (13, 74.8731), (14, 77.2421), (15, 79.4035), (16, 81.2479), (17, 82.9473), (18, 84.4161), (19, 85.7173), (20, 86.9588), (21, 88.0485), (22, 89.0458), (23, 89.9274), (24, 90.7616), (25, 91.4734), (26, 92.1717), (27, 92.7577), (28, 93.3077), (29, 93.8198), (30, 94.2513), (31, 94.6642), (32, 95.0477), (33, 95.398), (34, 95.7212), (35, 96.0196), (36, 96.2971), (37, 96.5436), (38, 96.7607), (39, 96.9959), (40, 97.1883), (41, 97.3738), (42, 97.5543), (43, 97.7168), (44, 97.8487), (45, 97.9881), (46, 98.1088), (47, 98.2109), (48, 98.3096), (49, 98.4162), (50, 98.5075), (51, 98.5984), (52, 98.6756), (53, 98.7461), (54, 98.8104), (55, 98.8668), (56, 98.9261), (57, 98.9875), (58, 99.0451), (59, 99.0913), (60, 99.1427), (61, 99.1861), (62, 99.2267), (63, 99.2667), (64, 99.3034), (65, 99.3333), (66, 99.3671), (67, 99.3942), (68, 99.4269), (69, 99.4546), (70, 99.4833), (71, 99.5014), (72, 99.52), (73, 99.5386), (74, 99.5668), (75, 99.5894), (76, 99.6091), (77, 99.6227), (78, 99.6368), (79, 99.6548), (80, 99.6644), (81, 99.674), (82, 99.6892), (83, 99.7011), (84, 99.7106), (85, 99.7208), (86, 99.731), (87, 99.7434), (88, 99.7513), (89, 99.7614), (90, 99.771), (91, 99.7766), (92, 99.7868), (93, 99.7947), (94, 99.8037), (95, 99.811), (96, 99.8195), (97, 99.8251), (98, 99.8336), (99, 99.8415), (100, 99.8438), (101, 99.846), (102, 99.8533), (103, 99.8584), (104, 99.8641), (105, 99.8686), (106, 99.8748), (107, 99.8799), (108, 99.8855), (109, 99.8906), (110, 99.8917), (111, 99.8962), (112, 99.9002), (113, 99.9024), (114, 99.9069), (115, 99.9098), (116, 99.9148), (117, 99.9165), (118, 99.9176), (119, 99.9199), (120, 99.9239), (121, 99.9261), (122, 99.9278), (123, 99.9312), (124, 99.9346), (125, 99.938), (126, 99.9385), (127, 99.9402), (128, 99.943), (129, 99.9453), (130, 99.9464), (131, 99.947), (132, 99.9487), (133, 99.9521), (134, 99.9526), (135, 99.9537), (136, 99.9554), (137, 99.9571), (138, 99.9577), (139, 99.9583), (140, 99.9605), (141, 99.9611), (142, 99.9616), (143, 99.9622), (144, 99.9628), (145, 99.9633), (146, 99.9645), (147, 99.9662), (148, 99.9684), (149, 99.9701), (150, 99.9712), (153, 99.9718), (154, 99.9735), (157, 99.9741), (159, 99.9746), (161, 99.9752), (163, 99.9757), (164, 99.9774), (165, 99.978), (166, 99.9791), (168, 99.9803), (170, 99.9808), (171, 99.9814), (173, 99.9825), (174, 99.9831), (177, 99.9836), (178, 99.9842), (179, 99.9848), (180, 99.9853), (181, 99.9859), (182, 99.987), (187, 99.9876), (189, 99.9887), (192, 99.9893), (197, 99.9904), (198, 99.991), (199, 99.9915), (200, 99.9921), (203, 99.9927), (204, 99.9932), (205, 99.9938), (215, 99.9944), (217, 99.9949), (219, 99.9955), (220, 99.9961), (227, 99.9972), (233, 99.9977), (253, 99.9983), (293, 99.9989), (304, 99.9994), (319, 100.0)]\n",
      "Ordered Second List Length Percentages: [(0, 0.0925), (1, 8.0517), (2, 6.689), (3, 7.1955), (4, 8.3066), (5, 7.6529), (6, 6.9456), (7, 6.1706), (8, 5.1638), (9, 4.4181), (10, 3.7988), (11, 3.3493), (12, 3.0125), (13, 2.757), (14, 2.3571), (15, 2.1417), (16, 1.907), (17, 1.7547), (18, 1.5111), (19, 1.4169), (20, 1.2883), (21, 1.1388), (22, 1.0474), (23, 0.9386), (24, 0.8585), (25, 0.7812), (26, 0.7445), (27, 0.6515), (28, 0.5906), (29, 0.5409), (30, 0.4648), (31, 0.4371), (32, 0.3971), (33, 0.3723), (34, 0.3418), (35, 0.3192), (36, 0.2877), (37, 0.2465), (38, 0.2363), (39, 0.2301), (40, 0.2138), (41, 0.1923), (42, 0.1878), (43, 0.1777), (44, 0.1534), (45, 0.1686), (46, 0.128), (47, 0.1083), (48, 0.123), (49, 0.1145), (50, 0.0908), (51, 0.088), (52, 0.0914), (53, 0.0914), (54, 0.0801), (55, 0.0604), (56, 0.0739), (57, 0.0632), (58, 0.0553), (59, 0.0502), (60, 0.0587), (61, 0.0502), (62, 0.0485), (63, 0.04), (64, 0.0395), (65, 0.0338), (66, 0.0395), (67, 0.0333), (68, 0.0389), (69, 0.0254), (70, 0.0288), (71, 0.0254), (72, 0.0259), (73, 0.0243), (74, 0.0271), (75, 0.0243), (76, 0.0259), (77, 0.018), (78, 0.018), (79, 0.0203), (80, 0.0152), (81, 0.0147), (82, 0.0209), (83, 0.018), (84, 0.0124), (85, 0.0141), (86, 0.0169), (87, 0.0118), (88, 0.0113), (89, 0.013), (90, 0.0096), (91, 0.0085), (92, 0.0118), (93, 0.0118), (94, 0.0102), (95, 0.0113), (96, 0.0096), (97, 0.0062), (98, 0.0085), (99, 0.0079), (100, 0.0045), (101, 0.0023), (102, 0.0079), (103, 0.0062), (104, 0.0045), (105, 0.0056), (106, 0.0073), (107, 0.0062), (108, 0.0068), (109, 0.0068), (110, 0.0011), (111, 0.0073), (112, 0.0062), (113, 0.0011), (114, 0.0045), (115, 0.0045), (116, 0.0023), (117, 0.0023), (118, 0.0011), (119, 0.0023), (120, 0.0017), (121, 0.0006), (122, 0.0017), (123, 0.0039), (124, 0.0028), (125, 0.0034), (126, 0.0017), (127, 0.0028), (128, 0.0034), (129, 0.0034), (130, 0.0006), (131, 0.0011), (132, 0.0028), (133, 0.0034), (134, 0.0011), (135, 0.0011), (136, 0.0006), (137, 0.0017), (138, 0.0006), (139, 0.0011), (140, 0.0034), (141, 0.0011), (143, 0.0006), (144, 0.0006), (145, 0.0011), (146, 0.0028), (147, 0.0017), (148, 0.0017), (149, 0.0023), (150, 0.0023), (152, 0.0011), (153, 0.0011), (154, 0.0017), (155, 0.0011), (156, 0.0006), (157, 0.0006), (158, 0.0017), (159, 0.0017), (160, 0.0006), (161, 0.0006), (163, 0.0011), (164, 0.0017), (165, 0.0017), (166, 0.0006), (168, 0.0017), (169, 0.0006), (170, 0.0006), (171, 0.0006), (172, 0.0006), (173, 0.0006), (176, 0.0011), (177, 0.0017), (178, 0.0011), (179, 0.0017), (181, 0.0006), (182, 0.0006), (183, 0.0006), (185, 0.0006), (187, 0.0006), (190, 0.0006), (192, 0.0006), (193, 0.0006), (197, 0.0006), (198, 0.0006), (199, 0.0006), (202, 0.0017), (203, 0.0011), (204, 0.0006), (206, 0.0011), (217, 0.0006), (219, 0.0006), (220, 0.0011), (224, 0.0011), (239, 0.0006), (280, 0.0006), (296, 0.0006), (303, 0.0006), (304, 0.0006), (313, 0.0006), (319, 0.0006), (367, 0.0006), (501, 0.0006)]\n",
      "Cumulative Second List Length Percentages: [(0, 0.0925), (1, 8.1442), (2, 14.8332), (3, 22.0286), (4, 30.3353), (5, 37.9882), (6, 44.9338), (7, 51.1044), (8, 56.2682), (9, 60.6863), (10, 64.4851), (11, 67.8344), (12, 70.847), (13, 73.604), (14, 75.9611), (15, 78.1028), (16, 80.0098), (17, 81.7645), (18, 83.2756), (19, 84.6925), (20, 85.9808), (21, 87.1196), (22, 88.167), (23, 89.1055), (24, 89.964), (25, 90.7452), (26, 91.4897), (27, 92.1412), (28, 92.7318), (29, 93.2727), (30, 93.7375), (31, 94.1746), (32, 94.5717), (33, 94.9439), (34, 95.2857), (35, 95.605), (36, 95.8927), (37, 96.1391), (38, 96.3755), (39, 96.6056), (40, 96.8194), (41, 97.0117), (42, 97.1995), (43, 97.3772), (44, 97.5306), (45, 97.6993), (46, 97.8273), (47, 97.9356), (48, 98.0586), (49, 98.1731), (50, 98.2639), (51, 98.3519), (52, 98.4432), (53, 98.5346), (54, 98.6147), (55, 98.6751), (56, 98.749), (57, 98.8121), (58, 98.8674), (59, 98.9176), (60, 98.9763), (61, 99.0265), (62, 99.075), (63, 99.115), (64, 99.1545), (65, 99.1883), (66, 99.2278), (67, 99.2611), (68, 99.3), (69, 99.3254), (70, 99.3542), (71, 99.3796), (72, 99.4055), (73, 99.4298), (74, 99.4568), (75, 99.4811), (76, 99.507), (77, 99.5251), (78, 99.5431), (79, 99.5634), (80, 99.5787), (81, 99.5933), (82, 99.6142), (83, 99.6322), (84, 99.6447), (85, 99.6588), (86, 99.6757), (87, 99.6875), (88, 99.6988), (89, 99.7118), (90, 99.7214), (91, 99.7298), (92, 99.7417), (93, 99.7535), (94, 99.7637), (95, 99.7749), (96, 99.7845), (97, 99.7907), (98, 99.7992), (99, 99.8071), (100, 99.8116), (101, 99.8139), (102, 99.8218), (103, 99.828), (104, 99.8325), (105, 99.8381), (106, 99.8455), (107, 99.8517), (108, 99.8584), (109, 99.8652), (110, 99.8663), (111, 99.8737), (112, 99.8799), (113, 99.881), (114, 99.8855), (115, 99.89), (116, 99.8923), (117, 99.8945), (118, 99.8957), (119, 99.8979), (120, 99.8996), (121, 99.9002), (122, 99.9019), (123, 99.9058), (124, 99.9086), (125, 99.912), (126, 99.9137), (127, 99.9165), (128, 99.9199), (129, 99.9233), (130, 99.9239), (131, 99.925), (132, 99.9278), (133, 99.9312), (134, 99.9323), (135, 99.9334), (136, 99.934), (137, 99.9357), (138, 99.9363), (139, 99.9374), (140, 99.9408), (141, 99.9419), (143, 99.9425), (144, 99.943), (145, 99.9442), (146, 99.947), (147, 99.9487), (148, 99.9504), (149, 99.9526), (150, 99.9549), (152, 99.956), (153, 99.9571), (154, 99.9588), (155, 99.96), (156, 99.9605), (157, 99.9611), (158, 99.9628), (159, 99.9645), (160, 99.965), (161, 99.9656), (163, 99.9667), (164, 99.9684), (165, 99.9701), (166, 99.9707), (168, 99.9724), (169, 99.9729), (170, 99.9735), (171, 99.9741), (172, 99.9746), (173, 99.9752), (176, 99.9763), (177, 99.978), (178, 99.9791), (179, 99.9808), (181, 99.9814), (182, 99.982), (183, 99.9825), (185, 99.9831), (187, 99.9836), (190, 99.9842), (192, 99.9848), (193, 99.9853), (197, 99.9859), (198, 99.9865), (199, 99.987), (202, 99.9887), (203, 99.9898), (204, 99.9904), (206, 99.9915), (217, 99.9921), (219, 99.9927), (220, 99.9938), (224, 99.9949), (239, 99.9955), (280, 99.9961), (296, 99.9966), (303, 99.9972), (304, 99.9977), (313, 99.9983), (319, 99.9989), (367, 99.9994), (501, 100.0)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "first_list_lengths = [len(pair[0]) for pair in train_pairs]\n",
    "second_list_lengths = [len(pair[1]) for pair in train_pairs]\n",
    "\n",
    "# Count occurrences of each length\n",
    "first_list_length_counts = Counter(first_list_lengths)\n",
    "second_list_length_counts = Counter(second_list_lengths)\n",
    "\n",
    "# Calculate percentages\n",
    "total_first_list = len(first_list_lengths)\n",
    "total_second_list = len(second_list_lengths)\n",
    "\n",
    "first_list_length_percentages = {length: count / total_first_list * 100 for length, count in first_list_length_counts.items()}\n",
    "second_list_length_percentages = {length: count / total_second_list * 100 for length, count in second_list_length_counts.items()}\n",
    "\n",
    "# Order by length\n",
    "ordered_first_list_percentages = sorted(first_list_length_percentages.items(), key=lambda x: x[0])\n",
    "ordered_second_list_percentages = sorted(second_list_length_percentages.items(), key=lambda x: x[0])\n",
    "\n",
    "# Calculate cumulative percentages\n",
    "cumulative_first_list_percentages = []\n",
    "cumulative_percentage = 0\n",
    "for length, percentage in ordered_first_list_percentages:\n",
    "    cumulative_percentage += percentage\n",
    "    cumulative_first_list_percentages.append((length, cumulative_percentage))\n",
    "\n",
    "cumulative_second_list_percentages = []\n",
    "cumulative_percentage = 0\n",
    "for length, percentage in ordered_second_list_percentages:\n",
    "    cumulative_percentage += percentage\n",
    "    cumulative_second_list_percentages.append((length, cumulative_percentage))\n",
    "\n",
    "# Print the ordered and cumulative percentages\n",
    "print(\"Ordered First List Length Percentages:\", [(length, round(percentage, 4)) for length, percentage in ordered_first_list_percentages])\n",
    "print(\"Cumulative First List Length Percentages:\", [(length, round(percentage, 4)) for length, percentage in cumulative_first_list_percentages])\n",
    "\n",
    "print(\"Ordered Second List Length Percentages:\", [(length, round(percentage, 4)) for length, percentage in ordered_second_list_percentages])\n",
    "print(\"Cumulative Second List Length Percentages:\", [(length, round(percentage, 4)) for length, percentage in cumulative_second_list_percentages])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers, dropout_p = 0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.dropout(self.embedding(input_seq))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden, decoder_input, max_length, target_tensor = None, teaching_force_ratio = 0.5):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_outputs = []\n",
    "        decoder_hidden = hidden \n",
    "\n",
    "        for i in range(max_length):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            #decoder_output = decoder_output.detach()\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            if target_tensor is not None and random.random() < teaching_force_ratio:\n",
    "                # Teacher forcing\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1).detach()\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48237\n",
      "48841\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(input_vocab)\n",
    "output_dim = len(target_vocab)\n",
    "emb_dim = 128\n",
    "hidden_dim = 128\n",
    "num_layers = 1\n",
    "MAX_LEN = 100\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "batch_size = 32  # Adjust the batch size as per your requirements\n",
    "print(input_dim)\n",
    "print(output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# # Clear GPU memory cache\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Iterate over all available GPUs\n",
    "# for i in range(torch.cuda.device_count()):\n",
    "#     current_device = torch.device(f'cuda:{i}')\n",
    "#     print(current_device)\n",
    "#     # Move to the current GPU\n",
    "#     with torch.cuda.device(current_device):\n",
    "#         # Iterate over model parameters and buffers\n",
    "#         for obj in list(torch.nn.Module().parameters()) + list(torch.nn.Module().buffers()):\n",
    "#             print(obj)\n",
    "#             if obj is not None and obj.is_cuda:\n",
    "#                 obj.data = None  # This will release the memory associated with the tensor\n",
    "\n",
    "# # Optionally, clear the GPU memory cache again\n",
    "# torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4853"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177292\n",
      "5541\n",
      "44324\n",
      "1386\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the encoder and decoder and move them to the appropriate device\n",
    "encoder = Encoder(input_dim, emb_dim, hidden_dim, num_layers).to(device)\n",
    "decoder = Decoder(output_dim, emb_dim, hidden_dim, num_layers).to(device)\n",
    "\n",
    "# Define the optimizer and move the parameters to the appropriate device\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss(ignore_index=target_vocab['<pad>'])\n",
    "\n",
    "\n",
    "# Create batches of train pairs\n",
    "train_batches = [train_pairs[i:i+batch_size] for i in range(0, len(train_pairs), batch_size)]\n",
    "val_batches = [val_pairs[i:i+batch_size] for i in range(0, len(val_pairs), batch_size)]\n",
    "print(len(train_pairs))\n",
    "print(len(train_batches))\n",
    "print(len(val_pairs))\n",
    "print(len(val_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 32.05s, Training Visited 10000 lines, Loss: 0.1909\n",
      "Time: 65.53s, Training Visited 20000 lines, Loss: 0.1844\n",
      "Time: 100.11s, Training Visited 30000 lines, Loss: 0.1804\n",
      "Time: 131.05s, Training Visited 40000 lines, Loss: 0.1777\n",
      "Time: 163.56s, Training Visited 50000 lines, Loss: 0.1760\n",
      "Time: 198.80s, Training Visited 60000 lines, Loss: 0.1746\n",
      "Time: 231.64s, Training Visited 70000 lines, Loss: 0.1736\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[150], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m input_indices \u001b[38;5;241m=\u001b[39m [[input_vocab[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m seq] \u001b[38;5;241m+\u001b[39m [input_vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m*\u001b[39m (input_max_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(seq)) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m input_seqs]\n\u001b[0;32m     31\u001b[0m target_indices \u001b[38;5;241m=\u001b[39m [[target_vocab[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m seq] \u001b[38;5;241m+\u001b[39m [target_vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m*\u001b[39m (target_max_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(seq)) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m target_seqs]\n\u001b[1;32m---> 33\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_indices)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     34\u001b[0m target_seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(target_indices)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# mask = (target_seq != target_vocab['<pad>']).to(device)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# print(input_seq.shape)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# print(target_seq.shape)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# print(mask.sum())\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "    total_mask = 0\n",
    "    counter = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in train_batches[:]:\n",
    "        counter += batch_size\n",
    "        if counter % 10000 < batch_size:\n",
    "            end_time = time.time()\n",
    "            time_diff = end_time - start_time\n",
    "            average_loss = total_loss / counter\n",
    "            print(f\"Time: {time_diff:.2f}s, Training Visited {counter // 10000 * 10000} lines, Loss: {average_loss:.4f}\")\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        # input_seqs = [pair[0] for pair in batch]\n",
    "        # target_seqs = [pair[1] for pair in batch]\n",
    "        # Add the <eos> token to the end of each input sequence\n",
    "        input_seqs = [pair[0] + ['<eos>'] for pair in batch]\n",
    "        # Add the <sos> token to the beginning and the <eos> token to the end of each target sequence\n",
    "        target_seqs = [['<sos>'] + pair[1] + ['<eos>'] for pair in batch]\n",
    "\n",
    "        input_max_len = max(len(seq) for seq in input_seqs)\n",
    "        target_max_len = max(len(seq) for seq in target_seqs)\n",
    "\n",
    "        input_indices = [[input_vocab[token] for token in seq] + [input_vocab['<pad>']] * (input_max_len - len(seq)) for seq in input_seqs]\n",
    "        target_indices = [[target_vocab[token] for token in seq] + [target_vocab['<pad>']] * (target_max_len - len(seq)) for seq in target_seqs]\n",
    "\n",
    "        input_seq = torch.tensor(input_indices).to(device)\n",
    "        target_seq = torch.tensor(target_indices).to(device)\n",
    "        # mask = (target_seq != target_vocab['<pad>']).to(device)\n",
    "\n",
    "        # print(input_seq.shape)\n",
    "        # print(target_seq.shape)\n",
    "        # print(mask.sum())\n",
    "        encoder_outputs, encoder_hidden = encoder(input_seq)\n",
    "        decoder_input = target_seq[:, 0].unsqueeze(1).to(device)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, decoder_input, min(MAX_LEN, target_max_len), target_tensor = target_seq, teaching_force_ratio = 0.5)\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_seq[:, :MAX_LEN].reshape(-1)\n",
    "        )\n",
    "\n",
    "        # loss = 0\n",
    "        # for token in range(target_seq.shape[1]):\n",
    "        #     decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "        #     # step_loss = criterion(decoder_output, target_seq[:, token])\n",
    "        #     step_loss = criterion(decoder_output[mask[:, token]], target_seq[:, token][mask[:, token]])\n",
    "        #     loss += step_loss\n",
    "        #     decoder_input = target_seq[:, token].unsqueeze(1)\n",
    "        total_loss += loss.item()\n",
    "        # total_mask += mask.sum()\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "    # average_loss = total_loss / total_mask\n",
    "    average_loss = total_loss / len(train_pairs)\n",
    "    # average_loss = total_loss / 10\n",
    "    end_time = time.time()\n",
    "    time_diff = end_time - start_time\n",
    "    print(f\"Time: {time_diff:.2f}s, Epoch: {epoch+1}, Training Loss: {average_loss:.4f}\")\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_mask = 0\n",
    "        counter = 0\n",
    "        for batch in val_batches[:]:\n",
    "            counter += batch_size\n",
    "            if counter % 10000 < batch_size:\n",
    "                end_time = time.time()\n",
    "                time_diff = end_time - start_time\n",
    "                average_loss = total_loss / counter\n",
    "                print(f\"Time: {time_diff:.2f}s, Validation Visited {counter // 10000 * 10000} lines, Loss: {average_loss:.4f}\")\n",
    "\n",
    "            # input_seqs = [pair[0] for pair in batch]\n",
    "            # target_seqs = [pair[1] for pair in batch]\n",
    "            # Add the <eos> token to the end of each input sequence\n",
    "            input_seqs = [pair[0] + ['<eos>'] for pair in batch]\n",
    "            # Add the <sos> token to the beginning and the <eos> token to the end of each target sequence\n",
    "            target_seqs = [['<sos>'] + pair[1] + ['<eos>'] for pair in batch]\n",
    "\n",
    "            input_max_len = max(len(seq) for seq in input_seqs)\n",
    "            target_max_len = max(len(seq) for seq in target_seqs)\n",
    "\n",
    "            input_indices = [[input_vocab[token] for token in seq] + [input_vocab['<pad>']] * (input_max_len - len(seq)) for seq in input_seqs]\n",
    "            target_indices = [[target_vocab[token] for token in seq] + [target_vocab['<pad>']] * (target_max_len - len(seq)) for seq in target_seqs]\n",
    "\n",
    "            input_seq = torch.tensor(input_indices).to(device)\n",
    "            target_seq = torch.tensor(target_indices).to(device)\n",
    "            # for simplicity, mask validation in the same way. Ignore all <pad>.\n",
    "            # mask = (target_seq != target_vocab['<pad>']).to(device)\n",
    "            encoder_outputs, encoder_hidden = encoder(input_seq)\n",
    "            decoder_input = torch.tensor([target_vocab[\"<sos>\"]] * input_seq.shape[0]).unsqueeze(1).to(device)\n",
    "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, decoder_input, min(MAX_LEN, target_max_len), target_tensor = None)\n",
    "            loss = criterion(\n",
    "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "                target_seq[:, :MAX_LEN].reshape(-1)\n",
    "            )\n",
    "\n",
    "            # loss = 0\n",
    "            # for token in range(target_seq.shape[1]):\n",
    "            #     decoder_output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "            #     # step_loss = criterion(decoder_output.squeeze(1), target_seq[:, token])\n",
    "            #     step_loss = criterion(decoder_output[mask[:, token]], target_seq[:, token][mask[:, token]])\n",
    "            #     loss += step_loss\n",
    "            #     decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(1)\n",
    "            total_loss += loss.item()\n",
    "            # total_mask += mask.sum()\n",
    "\n",
    "        # average_loss = total_loss / total_mask\n",
    "        average_loss = total_loss / len(train_pairs)\n",
    "        # average_loss = total_loss / 10\n",
    "        end_time = time.time()\n",
    "        time_diff = end_time - start_time\n",
    "        print(f\"Time: {time_diff:.2f}s, Epoch: {epoch+1}, Validation Loss: {average_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
